{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f0d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line if you prefer to silence pip’s output\n",
    "# %%capture  \n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def ensure_installed(pkg_spec: str):\n",
    "    \"\"\"\n",
    "    Import a package if it’s already available; otherwise install it,\n",
    "    so the current kernel can immediately import it.\n",
    "    \"\"\"\n",
    "    # Strip extras like >= / == so we can import the bare name\n",
    "    pkg_name = pkg_spec.split(\"==\")[0].split(\">=\")[0].split(\"<\")[0]\n",
    "    try:\n",
    "        importlib.import_module(pkg_name)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_spec])\n",
    "\n",
    "# ── Core libraries ────────────────────────────────────────────────────────────\n",
    "for package in [\n",
    "    \"unsloth\",     # main library\n",
    "    \"snac\",        # your extra requirement\n",
    "\n",
    "    # ─ Optional / helper deps used by Unsloth examples  ─\n",
    "    # Comment out anything you don’t need or already have\n",
    "    \"bitsandbytes\",\n",
    "    \"accelerate\",\n",
    "    \"xformers==0.0.29.post3\",\n",
    "    \"peft\",\n",
    "    \"trl\",\n",
    "    \"triton\",\n",
    "    \"cut_cross_entropy\",\n",
    "    \"unsloth_zoo\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\",\n",
    "    \"datasets>=3.4.1,<4.0.0\",\n",
    "    \"huggingface_hub>=0.34.0\",\n",
    "    \"hf_transfer\",\n",
    "]:\n",
    "    ensure_installed(package)\n",
    "\n",
    "print(\"✅ All required libraries are installed and ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f262c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"canopylabs/3b-hi-ft-research_release\",\n",
    "    # \"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/model_3000_steps_4096_vllm\",#\"canopylabs/3b-hi-ft-research_release\",\n",
    "    max_seq_length= 4096, # Choose any for long context!\n",
    "    dtype = None, # Select None for auto detection\n",
    "    load_in_4bit = True, # Select True for 4bit which reduces memory usage\n",
    "    full_finetuning=True\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you already have: model, tokenizer\n",
    "print(\"special_tokens_map:\", tokenizer.special_tokens_map)         # dict of special roles → tokens\n",
    "print(\"all_special_tokens:\", tokenizer.all_special_tokens)         # list of every special token\n",
    "print(\"all_special_ids:\", tokenizer.all_special_ids)               # their IDs\n",
    "\n",
    "print(\"additional_special_tokens:\", tokenizer.additional_special_tokens)\n",
    "print(\"additional_special_tokens_ids:\", tokenizer.additional_special_tokens_ids)\n",
    "\n",
    "# What did we add after load()\n",
    "print(\"added_vocab_size:\", len(tokenizer.get_added_vocab()))\n",
    "print(\"added_vocab_sample:\", list(tokenizer.get_added_vocab().items())[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458973de",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = [\"<pause>\", \"<breath>\", \"<sigh>\", \"<laugh>\", \"<gasp>\", \"<chuckle>\", \"<hmm..>\"]\n",
    "print({t: tokenizer.convert_tokens_to_ids(t) for t in TAGS})\n",
    "# any -100 / unk? then they’re NOT registered as special tokens yet\n",
    "print(\"unk_token_id:\", tokenizer.unk_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc68392",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pad:\", tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(\"bos:\", tokenizer.bos_token, tokenizer.bos_token_id)\n",
    "print(\"eos:\", tokenizer.eos_token, tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09657d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_TAGS = [\"<pause>\", \"<breath>\", \"<sigh>\", \"<laugh>\", \"<gasp>\", \"<hmm..>\"] \n",
    "missing = [t for t in EMOTION_TAGS if tokenizer.convert_tokens_to_ids(t) is None]\n",
    "if missing:\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": missing})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33921c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70506488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Set CWD to the data_dir\n",
    "os.chdir(\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac\")\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"metadata_with_tags.csv\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"PATH\", Audio())\n",
    "print(dataset[0][\"PATH\"])  # Now will work since CWD == data_dir\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6924751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac\"\n",
    "print(os.path.isfile(os.path.join(data_dir, \"metadata_with_tags.csv\")))             # Should be True\n",
    "print(os.path.isdir(os.path.join(data_dir, \"SPEECHRIV\")))                              # Should be True\n",
    "print(os.path.isfile(os.path.join(data_dir, \"SPEECHRIV/1_AGENT.wav\")))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, Audio\n",
    "import locale\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "from snac import SNAC\n",
    "\n",
    "# Dataset loading\n",
    "os.chdir(\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac\")\n",
    "dataset = load_dataset(\"csv\", data_files=\"metadata_with_tags.csv\", split=\"train\")\n",
    "dataset = dataset.cast_column(\"PATH\", Audio())\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"First sample keys: {dataset[0].keys()}\")\n",
    "\n",
    "# SNAC model setup\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "ds_sample_rate = dataset[0][\"PATH\"][\"sampling_rate\"]\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
    "snac_model = snac_model.to(\"cuda\")\n",
    "\n",
    "# APPROACH 1: Original Orpheus approach with hardcoded offsets\n",
    "# This is what the original code uses\n",
    "def tokenise_audio_original_approach(waveform):\n",
    "    \"\"\"Original approach with hardcoded offsets\"\"\"\n",
    "    waveform = torch.from_numpy(waveform).unsqueeze(0).to(dtype=torch.float32)\n",
    "    resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)\n",
    "    waveform = resample_transform(waveform)\n",
    "    waveform = waveform.unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        codes = snac_model.encode(waveform)\n",
    "    \n",
    "    # Original offset scheme - each codebook gets its own range\n",
    "    all_codes = []\n",
    "    for i in range(codes[0].shape[1]):\n",
    "        all_codes.append(codes[0][0][i].item() + 128266)           # Codebook 0\n",
    "        all_codes.append(codes[1][0][2*i].item() + 128266 + 4096)   # Codebook 1, first\n",
    "        all_codes.append(codes[2][0][4*i].item() + 128266 + (2*4096))     # Codebook 2, first\n",
    "        all_codes.append(codes[2][0][(4*i)+1].item() + 128266 + (3*4096)) # Codebook 2, second\n",
    "        all_codes.append(codes[1][0][(2*i)+1].item() + 128266 + (4*4096)) # Codebook 1, second\n",
    "        all_codes.append(codes[2][0][(4*i)+2].item() + 128266 + (5*4096)) # Codebook 2, third\n",
    "        all_codes.append(codes[2][0][(4*i)+3].item() + 128266 + (6*4096)) # Codebook 2, fourth\n",
    "    \n",
    "    return all_codes\n",
    "\n",
    "# APPROACH 2: Dynamic tokenizer approach (your attempted modification)\n",
    "def tokenise_audio_dynamic_approach(waveform):\n",
    "    \"\"\"Your approach - raw codes without offsets\"\"\"\n",
    "    waveform = torch.from_numpy(waveform).unsqueeze(0).to(dtype=torch.float32)\n",
    "    resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)\n",
    "    waveform = resample_transform(waveform)\n",
    "    waveform = waveform.unsqueeze(0).to(\"cuda\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        codes = snac_model.encode(waveform)\n",
    "    \n",
    "    # Just raw codes, no offsets\n",
    "    all_codes = []\n",
    "    for i in range(codes[0].shape[1]):\n",
    "        all_codes.append(codes[0][0][i].item())\n",
    "        all_codes.append(codes[1][0][2*i].item())\n",
    "        all_codes.append(codes[2][0][4*i].item())\n",
    "        all_codes.append(codes[2][0][(4*i)+1].item())\n",
    "        all_codes.append(codes[1][0][(2*i)+1].item())\n",
    "        all_codes.append(codes[2][0][(4*i)+2].item())\n",
    "        all_codes.append(codes[2][0][(4*i)+3].item())\n",
    "    \n",
    "    return all_codes\n",
    "\n",
    "# CHOOSE YOUR APPROACH HERE\n",
    "USE_ORIGINAL_APPROACH = True  # Set to False to use dynamic approach\n",
    "\n",
    "if USE_ORIGINAL_APPROACH:\n",
    "    print(\"Using ORIGINAL Orpheus approach with hardcoded offsets\")\n",
    "    tokenise_audio = tokenise_audio_original_approach\n",
    "    \n",
    "    # For original approach, we need to ensure tokenizer vocabulary is large enough\n",
    "    # The max token ID will be around 128266 + 7*4096 = 156938\n",
    "    MAX_TOKEN_ID = 128266 + (7 * 4096)\n",
    "    \n",
    "    # Resize model embeddings if needed\n",
    "    if len(tokenizer) < MAX_TOKEN_ID:\n",
    "        print(f\"Resizing tokenizer from {len(tokenizer)} to {MAX_TOKEN_ID}\")\n",
    "        # Add dummy tokens to reach the required size\n",
    "        num_to_add = MAX_TOKEN_ID - len(tokenizer)\n",
    "        dummy_tokens = [f\"<dummy_{i}>\" for i in range(num_to_add)]\n",
    "        tokenizer.add_tokens(dummy_tokens)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # No need to set AUDIO_TOKENS_START for original approach\n",
    "    \n",
    "else:\n",
    "    print(\"Using DYNAMIC approach with tokenizer-added audio tokens\")\n",
    "    tokenise_audio = tokenise_audio_dynamic_approach\n",
    "    \n",
    "    # For dynamic approach, we need to add audio tokens to tokenizer\n",
    "    # First, tokenize one sample to find the max code value\n",
    "    test_audio = dataset[0][\"PATH\"][\"array\"]\n",
    "    test_codes = tokenise_audio(test_audio)\n",
    "    max_code = max(test_codes)\n",
    "    \n",
    "    # SNAC typically has 4096 codes per codebook, 3 codebooks\n",
    "    # But let's calculate from data to be sure\n",
    "    print(f\"Max code value found: {max_code}\")\n",
    "    num_audio_codes = max_code + 1\n",
    "    \n",
    "    # Add audio tokens to tokenizer\n",
    "    audio_tokens = [f\"<audio_{i}>\" for i in range(num_audio_codes)]\n",
    "    tokenizer.add_tokens(audio_tokens)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    AUDIO_TOKENS_START = len(tokenizer) - num_audio_codes\n",
    "    print(f\"AUDIO_TOKENS_START: {AUDIO_TOKENS_START}\")\n",
    "\n",
    "# Process dataset\n",
    "def add_codes(example):\n",
    "    codes_list = None\n",
    "    try:\n",
    "        answer_audio = example.get(\"PATH\")\n",
    "        if answer_audio and \"array\" in answer_audio:\n",
    "            audio_array = answer_audio[\"array\"]\n",
    "            codes_list = tokenise_audio(audio_array)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row due to error: {e}\")\n",
    "    example[\"codes_list\"] = codes_list\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_codes, remove_columns=[\"PATH\"])\n",
    "dataset = dataset.filter(lambda x: x[\"codes_list\"] is not None)\n",
    "dataset = dataset.filter(lambda x: len(x[\"codes_list\"]) > 0)\n",
    "\n",
    "# Remove duplicate frames\n",
    "def remove_duplicate_frames(example):\n",
    "    vals = example[\"codes_list\"]\n",
    "    if len(vals) % 7 != 0:\n",
    "        raise ValueError(\"Input list length must be divisible by 7\")\n",
    "    result = vals[:7]\n",
    "    for i in range(7, len(vals), 7):\n",
    "        if vals[i] != result[-7]:\n",
    "            result.extend(vals[i:i+7])\n",
    "    example[\"codes_list\"] = result\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(remove_duplicate_frames)\n",
    "\n",
    "# Define special tokens\n",
    "tokeniser_length = 128256\n",
    "start_of_text = 128000\n",
    "end_of_text = 128009\n",
    "start_of_speech = tokeniser_length + 1\n",
    "end_of_speech = tokeniser_length + 2\n",
    "start_of_human = tokeniser_length + 3\n",
    "end_of_human = tokeniser_length + 4\n",
    "start_of_ai = tokeniser_length + 5\n",
    "end_of_ai = tokeniser_length + 6\n",
    "pad_token = tokeniser_length + 7\n",
    "\n",
    "# Set tokenizer pad token\n",
    "tokenizer.pad_token_id = pad_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create input IDs\n",
    "def create_input_ids(example):\n",
    "    # Use the correct field name for text\n",
    "    text_field = \"TEXT\" if \"TEXT\" in example else \"text\"\n",
    "    text_prompt = example[text_field]\n",
    "    \n",
    "    # Handle multi-speaker if source field exists\n",
    "    if \"source\" in example:\n",
    "        text_prompt = f\"{example['source']}: {text_prompt}\"\n",
    "    \n",
    "    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)\n",
    "    text_ids.append(end_of_text)\n",
    "    \n",
    "    # Handle audio codes based on approach\n",
    "    if USE_ORIGINAL_APPROACH:\n",
    "        # Original: codes are already offset in tokenise_audio\n",
    "        audio_token_ids = example[\"codes_list\"]\n",
    "    else:\n",
    "        # Dynamic: add offset here\n",
    "        audio_token_ids = [AUDIO_TOKENS_START + code for code in example[\"codes_list\"]]\n",
    "    \n",
    "    input_ids = (\n",
    "        [start_of_human]\n",
    "        + text_ids\n",
    "        + [end_of_human]\n",
    "        + [start_of_ai]\n",
    "        + [start_of_speech]\n",
    "        + audio_token_ids\n",
    "        + [end_of_speech]\n",
    "        + [end_of_ai]\n",
    "    )\n",
    "    # max_len = 2048\n",
    "    # if len(input_ids) > max_len:\n",
    "    #     input_ids = input_ids[:max_len]\n",
    "    example[\"input_ids\"] = input_ids\n",
    "    example[\"labels\"] = input_ids.copy()\n",
    "    example[\"attention_mask\"] = [1] * len(input_ids)\n",
    "    return example\n",
    "\n",
    "# Map and clean dataset\n",
    "text_column = \"TEXT\" if \"TEXT\" in dataset.column_names else \"text\"\n",
    "remove_cols = [text_column, \"codes_list\"]\n",
    "if \"source\" in dataset.column_names:\n",
    "    remove_cols.append(\"source\")\n",
    "\n",
    "dataset = dataset.map(create_input_ids, remove_columns=remove_cols)\n",
    "\n",
    "# Keep only necessary columns\n",
    "columns_to_keep = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "columns_to_remove = [col for col in dataset.column_names if col not in columns_to_keep]\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "print(f\"Final dataset size: {len(dataset)}\")\n",
    "print(f\"Sample input_ids length: {len(dataset[0]['input_ids'])}\")\n",
    "print(f\"Max token ID in dataset: {max(max(sample['input_ids']) for sample in dataset)}\")\n",
    "\n",
    "# Verify token IDs are within vocabulary\n",
    "max_token_in_data = max(max(sample['input_ids']) for sample in dataset)\n",
    "if max_token_in_data >= len(tokenizer):\n",
    "    print(f\"ERROR: Max token ID {max_token_in_data} >= vocab size {len(tokenizer)}\")\n",
    "    print(\"Need to resize tokenizer/model embeddings!\")\n",
    "else:\n",
    "    print(f\"✓ All token IDs within vocabulary (max: {max_token_in_data}, vocab: {len(tokenizer)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dd058",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = [\"<pause>\",\"<breath>\",\"<sigh>\", \"<hmm..>\", \"<laugh>\"]\n",
    "print({t: tokenizer.convert_tokens_to_ids(t) for t in TAGS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc515784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0][\"input_ids\"])\n",
    "print(dataset[0][\"labels\"])\n",
    "print(dataset[0][\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50276644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Optionally, also cast new embeddings\n",
    "model.get_input_embeddings().weight.data = model.get_input_embeddings().weight.data.to(model.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# Disable Unsloth's problematic optimization\n",
    "# os.environ[\"UNSLOTH_USE_FUSED_CROSS_ENTROPY\"] = \"0\"\n",
    "\n",
    "\n",
    "model = model.to(dtype=torch.bfloat16)\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3, # Set this for 1 full training run.\n",
    "        max_steps = 4000,\n",
    "        learning_rate = 1.5e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_700\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        fp16=False, \n",
    "        bf16=True\n",
    "    ),\n",
    "    # data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea676b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ff042",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8916f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3881b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # \"ठीक है, मैं कल आपके लिए demo arrange कर दूँगा क्या आपके पास और सवाल हैं?\",\n",
    "    # \"मुझे quick demo चाहिए, <breath> बस 2 minute का, right now.\" \n",
    "#     ''' \n",
    "# बिल्कुल, पहले ऐप खोलें और 'Add New Beneficiary' चुनें। बच्चे का नाम, उम्र (महीनों में) <hmm..>, \n",
    "\n",
    "\n",
    "# '''\n",
    "# \"हाँ, <hmm..> मैंने पहली बार आपके product के बारे में एक webinar में सुना था मैं समझना चाहता हूँ कि यह हमारी existing systems के साथ कैसे integrate कर सकता है \" \n",
    "# \"ज़रूर <happy> हमारी pricing flexible plans पर आधारित है, <hmm..> ताकि आपके business needs के अनुसार best fit हो सके \"\n",
    "\"Delhi की एक retail chain ने हमारे solutions से अपनी sales में 30% तक वृद्धि देखी है। <breath> उनका feedback बहुत encouraging रहा है ।\"\n",
    "# \"ये तो बहुत flexible है। <hmm..> क्या मैं इसमें social media links भी जोड़ सकता हूँ?\"\n",
    "]\n",
    "\n",
    "chosen_voice = None # None for single-speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run Inference\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# Moving snac_model cuda to cpu\n",
    "snac_model.to(\"cpu\")\n",
    "\n",
    "prompts_ = [(f\"{chosen_voice}: \" + p) if chosen_voice else p for p in prompts]\n",
    "\n",
    "all_input_ids = []\n",
    "\n",
    "for prompt in prompts_:\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "  all_input_ids.append(input_ids)\n",
    "\n",
    "start_token = torch.tensor([[ 128259]], dtype=torch.int64) # Start of human\n",
    "end_tokens = torch.tensor([[128009, 128260]], dtype=torch.int64) # End of text, End of human\n",
    "\n",
    "all_modified_input_ids = []\n",
    "for input_ids in all_input_ids:\n",
    "  modified_input_ids = torch.cat([start_token, input_ids, end_tokens], dim=1) # SOH SOT Text EOT EOH\n",
    "  all_modified_input_ids.append(modified_input_ids)\n",
    "\n",
    "all_padded_tensors = []\n",
    "all_attention_masks = []\n",
    "max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
    "for modified_input_ids in all_modified_input_ids:\n",
    "  padding = max_length - modified_input_ids.shape[1]\n",
    "  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
    "  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
    "  all_padded_tensors.append(padded_tensor)\n",
    "  all_attention_masks.append(attention_mask)\n",
    "\n",
    "all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
    "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "input_ids = all_padded_tensors.to(\"cuda\")\n",
    "attention_mask = all_attention_masks.to(\"cuda\")\n",
    "generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      max_new_tokens=2000,\n",
    "      do_sample=True,\n",
    "      temperature=0.4,\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.1,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=128258,\n",
    "     use_cache = True\n",
    "  )\n",
    "token_to_find = 128257\n",
    "token_to_remove = 128258\n",
    "\n",
    "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
    "\n",
    "if len(token_indices[1]) > 0:\n",
    "    last_occurrence_idx = token_indices[1][-1].item()\n",
    "    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
    "else:\n",
    "    cropped_tensor = generated_ids\n",
    "\n",
    "mask = cropped_tensor != token_to_remove\n",
    "\n",
    "processed_rows = []\n",
    "\n",
    "for row in cropped_tensor:\n",
    "    masked_row = row[row != token_to_remove]\n",
    "    processed_rows.append(masked_row)\n",
    "\n",
    "code_lists = []\n",
    "\n",
    "for row in processed_rows:\n",
    "    row_length = row.size(0)\n",
    "    new_length = (row_length // 7) * 7\n",
    "    trimmed_row = row[:new_length]\n",
    "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
    "    code_lists.append(trimmed_row)\n",
    "\n",
    "\n",
    "def redistribute_codes(code_list):\n",
    "  layer_1 = []\n",
    "  layer_2 = []\n",
    "  layer_3 = []\n",
    "  for i in range((len(code_list)+1)//7):\n",
    "    layer_1.append(code_list[7*i])\n",
    "    layer_2.append(code_list[7*i+1]-4096)\n",
    "    layer_3.append(code_list[7*i+2]-(2*4096))\n",
    "    layer_3.append(code_list[7*i+3]-(3*4096))\n",
    "    layer_2.append(code_list[7*i+4]-(4*4096))\n",
    "    layer_3.append(code_list[7*i+5]-(5*4096))\n",
    "    layer_3.append(code_list[7*i+6]-(6*4096))\n",
    "  codes = [torch.tensor(layer_1).unsqueeze(0),\n",
    "         torch.tensor(layer_2).unsqueeze(0),\n",
    "         torch.tensor(layer_3).unsqueeze(0)]\n",
    "\n",
    "  # codes = [c.to(\"cuda\") for c in codes]\n",
    "  audio_hat = snac_model.decode(codes)\n",
    "  return audio_hat\n",
    "\n",
    "my_samples = []\n",
    "for code_list in code_lists:\n",
    "  samples = redistribute_codes(code_list)\n",
    "  my_samples.append(samples)\n",
    "from IPython.display import display, Audio\n",
    "if len(prompts) != len(my_samples):\n",
    "  raise Exception(\"Number of prompts and samples do not match\")\n",
    "else:\n",
    "  for i in range(len(my_samples)):\n",
    "    print(prompts[i])\n",
    "    samples = my_samples[i]\n",
    "    display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))\n",
    "# Clean up to save RAM\n",
    "del my_samples,samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Assuming `tokenizer` is your extended tokenizer\n",
    "tokenizer.save_pretrained(\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/outputs_700/checkpoint-4000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/outputs_700/checkpoint-4000\", torch_dtype=\"bfloat16\", device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/outputs_700/checkpoint-4000\")\n",
    "model.save_pretrained(\"model_4000_fft\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"model_4000_fft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "# if False:\n",
    "#     model.push_to_hub(\"hf/model\", token = \"\")\n",
    "#     tokenizer.push_to_hub(\"hf/model\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b1bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained_merged(\"model_5000\", tokenizer, save_method=\"merged_16bit\") #does not work as its full fine tuning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde1a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "# from safetensors import safe_open\n",
    "# import json, os\n",
    "\n",
    "# SRC = \"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/model_5000_steps_4096\"          # your folder (screenshot)\n",
    "# DST = \"./model_5000_steps_4096_vllm\"     # output fixed folder\n",
    "\n",
    "# # 1) Find the *actual* rows in the checkpoint’s embed matrix\n",
    "# idx = json.load(open(os.path.join(SRC, \"model.safetensors.index.json\")))\n",
    "# embed_shard = idx[\"weight_map\"][\"model.embed_tokens.weight\"]\n",
    "# with safe_open(os.path.join(SRC, embed_shard), framework=\"pt\") as f:\n",
    "#     OLD_ROWS = f.get_tensor(\"model.embed_tokens.weight\").shape[0]   # -> 156940\n",
    "\n",
    "# # 2) Required vocab from tokenizer (includes your new tags)\n",
    "# tok = AutoTokenizer.from_pretrained(SRC)\n",
    "# REQUIRED = max(tok.get_vocab().values()) + 1                        # -> 156946\n",
    "\n",
    "# # 3) Load with a temp config that MATCHES the checkpoint (so weights load)\n",
    "# cfg = AutoConfig.from_pretrained(SRC)\n",
    "# cfg.vocab_size = OLD_ROWS\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     SRC, config=cfg, device_map=\"cpu\", low_cpu_mem_usage=True\n",
    "# )\n",
    "\n",
    "# # 4) Now grow embeddings to cover the new tags (preserves old rows)\n",
    "# if model.get_input_embeddings().weight.shape[0] != REQUIRED:\n",
    "#     model.resize_token_embeddings(REQUIRED)\n",
    "# model.config.vocab_size = REQUIRED\n",
    "\n",
    "# # 5) Keep pad/eos sane\n",
    "# model.config.pad_token_id = tok.pad_token_id\n",
    "# try:\n",
    "#     model.generation_config.pad_token_id = tok.pad_token_id\n",
    "#     model.generation_config.eos_token_id = tok.eos_token_id\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# # 6) Save a vLLM-ready export\n",
    "# os.makedirs(DST, exist_ok=True)\n",
    "# model.save_pretrained(DST, safe_serialization=True)\n",
    "# tok.save_pretrained(DST)\n",
    "\n",
    "# # 7) Verify\n",
    "# check = AutoModelForCausalLM.from_pretrained(DST, device_map=\"cpu\")\n",
    "# rows = check.get_input_embeddings().weight.shape[0]\n",
    "# print(\"rows=\", rows, \"vocab_size=\", check.config.vocab_size)  # both must be 156946\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from orpheus_tts import OrpheusModel\n",
    "import wave\n",
    "import time\n",
    "import os \n",
    "from IPython.display import Audio\n",
    "# Use the path to your local folder (relative or absolute)\n",
    "model = OrpheusModel(model_name=\"/home/user/voice/Orpheus-TTS/finetune/hf_cache/datasets--telecmiusa--tts-hi-data/snapshots/d564239b4542d4e25ee213660bf0104e700858ac/model_4000_fft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc07a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"हाँ, <hmm..> मैंने पहली बार आपके product के बारे में एक webi-nar में सुना था मैं समझना चाहता हूँ कि यह हमारी existing systems के साथ कैसे integrate कर सकता है \" \n",
    "prompt =\"Delhi की एक retail chain ने हमारे solutions से अपनी sales में 30% तक वृद्धि देखी है। <hmm..> उनका feedback बहुत encouraging रहा है ।\"\n",
    " \n",
    "# prompt = '''Absolutely <happy> हमारे system में team performance के लिए भी कई analytics tools मौजूद हैं <pause> आप individual \n",
    "# और team performance both track कर सकते हैं और rewards और training strategies accordingly plan कर सकते हैं <hmm..> \n",
    "# क्या आप training और support options के बारे में कुछ जानना चाहेंगे'''\n",
    "# prompt = prompt + \" \" + \"<chuckle\"\n",
    "\n",
    "\n",
    "filename = \"prompt_8.wav\"\n",
    "start_time = time.monotonic()\n",
    "syn_tokens = model.generate_speech(\n",
    "   prompt=prompt,\n",
    "   voice=None,\n",
    "   )\n",
    "\n",
    "with wave.open(filename, \"wb\") as wf:\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(2)\n",
    "    wf.setframerate(24000)\n",
    "\n",
    "    total_frames = 0\n",
    "    chunk_counter = 0\n",
    "    for audio_chunk in syn_tokens:  # output streaming\n",
    "        chunk_counter += 1\n",
    "        frame_count = len(audio_chunk) // (wf.getsampwidth() * wf.getnchannels())\n",
    "        total_frames += frame_count\n",
    "        wf.writeframes(audio_chunk)\n",
    "    duration = total_frames / wf.getframerate()\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(f\"It took {end_time - start_time} seconds to generate {duration:.2f} seconds of audio\")\n",
    "Audio(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837ba5b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (orpheus)",
   "language": "python",
   "name": "orpheus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
